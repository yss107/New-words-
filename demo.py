#!/usr/bin/env python3
"""
Demonstration script showing the complete Next Word Prediction system
"""
import os
import sys
import time

def print_header(title):
    """Print a formatted header"""
    print("\n" + "="*60)
    print(f" {title}")
    print("="*60)

def demonstrate_system():
    """Run a complete demonstration"""
    print_header("NEXT WORD PREDICTION SYSTEM DEMONSTRATION")
    
    print("\nThis system implements a neural network-based next word prediction")
    print("model similar to GPT, but simpler. It learns from text data and")
    print("predicts the most likely next word given a sequence of input words.")
    
    # Import here to avoid early imports
    from text_processor import TextPreprocessor, DatasetGenerator
    from neural_network import NextWordPredictor, ModelTrainer
    
    print_header("STEP 1: TEXT ANALYSIS")
    
    # Load and analyze text
    with open('book.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    
    preprocessor = TextPreprocessor()
    cleaned_text = preprocessor.clean_text(text)
    words = preprocessor.tokenize(cleaned_text)
    preprocessor.build_vocabulary(words)
    analysis = preprocessor.analyze_text(words)
    
    print(f"üìñ Source: The Adventures of Sherlock Holmes")
    print(f"üìä Total words: {analysis['total_words']:,}")
    print(f"üìù Unique words: {analysis['unique_words']:,}")
    print(f"üìè Average word length: {analysis['avg_word_length']:.1f} characters")
    
    print(f"\nüî§ Most frequent words:")
    for i, (word, count) in enumerate(analysis['most_common_words'][:5], 1):
        print(f"   {i}. '{word}' appears {count} times")
    
    print(f"\nüîó Most common word pairs:")
    for i, ((w1, w2), count) in enumerate(analysis['most_common_pairs'][:5], 1):
        print(f"   {i}. '{w1} {w2}' appears {count} times")
    
    print_header("STEP 2: TRAINING DATA PREPARATION")
    
    # Prepare training data
    generator = DatasetGenerator(sequence_length=5)
    input_sequences, output_words = generator.create_sequences(words)
    encoded_inputs = generator.encode_sequences(input_sequences, preprocessor.word_to_id)
    encoded_outputs = generator.encode_words(output_words, preprocessor.word_to_id)
    
    print(f"üîÑ Sequence length: 5 words")
    print(f"üìã Training sequences created: {len(encoded_inputs):,}")
    print(f"üéØ Vocabulary size: {len(preprocessor.word_to_id):,}")
    
    print(f"\nüìù Example training sequence:")
    print(f"   Input:  {' '.join(input_sequences[0])}")
    print(f"   Output: {output_words[0]}")
    
    print_header("STEP 3: NEURAL NETWORK TRAINING")
    
    print("üß† Building LSTM neural network...")
    print("   ‚Ä¢ Embedding layer (50 dimensions)")
    print("   ‚Ä¢ LSTM layer (128 units)")
    print("   ‚Ä¢ Dense layer (128 units)")
    print("   ‚Ä¢ Dropout (30%)")
    print("   ‚Ä¢ Output layer (softmax)")
    
    # Create and train model (quick version for demo)
    trainer = ModelTrainer(len(preprocessor.word_to_id), sequence_length=5)
    
    print(f"\nüöÄ Training model (quick demo with 3 epochs)...")
    results = trainer.train_model(
        encoded_inputs, encoded_outputs, 
        epochs=3, batch_size=16
    )
    
    accuracy = results['evaluation']['accuracy']
    print(f"\n‚úÖ Training completed!")
    print(f"   Final accuracy: {accuracy:.1%}")
    print(f"   (Note: Low accuracy is normal with only 3 epochs)")
    
    print_header("STEP 4: PREDICTION DEMONSTRATION")
    
    predictor = results['predictor']
    
    # Test sentences from the training data domain
    test_sentences = [
        "holmes sat up in his chair",
        "the criminal shows considerable ingenuity and", 
        "watson walked into the room and",
        "the morning sun cast long shadows",
        "in the smoking room of the"
    ]
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\nüîÆ Test {i}: '{sentence}'")
        
        # Tokenize and prepare
        test_words = preprocessor.tokenize(sentence)
        if len(test_words) >= 5:
            input_seq = test_words[-5:]
            encoded_seq = [preprocessor.word_to_id.get(w, 0) for w in input_seq]
            
            # Predict
            predictions = predictor.predict_next_words(encoded_seq, top_k=3)
            
            print(f"   üì• Input: {' '.join(input_seq)}")
            print(f"   üì§ Predicted next words:")
            
            for j, (word_id, prob) in enumerate(predictions, 1):
                word = preprocessor.id_to_word.get(word_id, '<unknown>')
                print(f"      {j}. '{word}' ({prob:.1%})")
    
    print_header("SYSTEM CAPABILITIES")
    
    print("‚ú® What this system demonstrates:")
    print("   ‚Ä¢ Text preprocessing and analysis")
    print("   ‚Ä¢ Vocabulary building and word encoding")
    print("   ‚Ä¢ LSTM neural network architecture")
    print("   ‚Ä¢ Sequence-to-word prediction")
    print("   ‚Ä¢ Probability distributions over vocabulary")
    print("   ‚Ä¢ Model training and evaluation pipeline")
    
    print("\nüöÄ How to use:")
    print("   ‚Ä¢ Run 'python main.py' for full training (30 epochs)")
    print("   ‚Ä¢ Run 'python test_system.py' for quick validation")
    print("   ‚Ä¢ Modify hyperparameters in the code")
    print("   ‚Ä¢ Replace 'book.txt' with your own text data")
    
    print("\nüìà For better results:")
    print("   ‚Ä¢ Train for more epochs (30-50)")
    print("   ‚Ä¢ Use larger text datasets")
    print("   ‚Ä¢ Experiment with model architecture")
    print("   ‚Ä¢ Adjust sequence length")
    
    print_header("DEMONSTRATION COMPLETE")
    print("üéâ Next Word Prediction system successfully demonstrated!")
    print("   Check the README.md for detailed usage instructions.")
    
    return True

if __name__ == "__main__":
    try:
        demonstrate_system()
    except Exception as e:
        print(f"\n‚ùå Error during demonstration: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)